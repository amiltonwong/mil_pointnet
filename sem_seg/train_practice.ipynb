{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import socket\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/data/code3/mil_pointnet/sem_seg\"#os.path.dirname(os.path.abspath(__file__))\n",
    "ROOT_DIR = os.path.dirname(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/data/code3/mil_pointnet/sem_seg', '/data/code3/mil_pointnet')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_DIR, ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/code3/mil_pointnet/utils'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(ROOT_DIR, 'utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(ROOT_DIR, 'utils'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Namespace(batch_size=24, decay_rate=0.5, decay_step=300000, gpu=0, learning_rate=0.001, log_dir='log', max_epoch=50, momentum=0.9, num_point=4096, optimizer='adam', test_area=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parmas:\n",
    "BATCH_SIZE = 24 # help='Batch Size during training [default: 24]')\n",
    "NUM_POINT = 4096 #, help='Point number [default: 4096]')\n",
    "MAX_EPOCH = 50 #help='Epoch to run [default: 50]')\n",
    "BASE_LEARNING_RATE = 0.001 #, help='Initial learning rate [default: 0.001]'\n",
    "GPU_INDEX = 0 #, help='GPU to use [default: GPU 0]')# \n",
    "MOMENTUM = 0.9 #, help='Initial learning rate [default: 0.9]')\n",
    "OPTIMIZER = 'adam' #, help='adam or momentum [default: adam]')\n",
    "DECAY_STEP = 300000 #, help='Decay step for lr decay [default: 300000]')\n",
    "DECAY_RATE = 0.5 #, help='Decay rate for lr decay [default: 0.5]')FLAGS.decay_rate\n",
    "TEST_AREA = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = 'log180617'#, help='Log dir [default: log]')\n",
    "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
    "os.system('cp model.py %s' % (LOG_DIR)) # bkp of model def\n",
    "os.system('cp train.py %s' % (LOG_DIR)) # bkp of train procedure\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
    "#LOG_FOUT.write(str(FLAGS)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_POINT = 4096\n",
    "NUM_CLASSES = 13\n",
    "\n",
    "# batch_norm params\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "#BN_DECAY_DECAY_STEP = float(DECAY_STEP * 2)\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'milton-ThinkCentre-M93p'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "socket.gethostname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOSTNAME = socket.gethostname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FILES = provider.getDataFiles('indoor3d_sem_seg_hdf5_data/all_files.txt')\n",
    "room_filelist = [line.rstrip() for line in open('indoor3d_sem_seg_hdf5_data/room_filelist.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23585"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(room_filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['indoor3d_sem_seg_hdf5_data/ply_data_all_0.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_1.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_2.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_3.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_4.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_5.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_6.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_7.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_8.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_9.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_10.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_11.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_12.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_13.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_14.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_15.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_16.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_17.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_18.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_19.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_20.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_21.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_22.h5',\n",
       " 'indoor3d_sem_seg_hdf5_data/ply_data_all_23.h5']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def load_h5(h5_filename):\n",
    "    f = h5py.File(h5_filename)\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    return (data, label)\"\"\"\n",
    "f = h5py.File(ALL_FILES[0])\n",
    "data = f['data'][:]\n",
    "label = f['label'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585, 4096, 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(585, 4096)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(label[:,:]), np.min(label[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((1000, 4096, 9), (1000, 4096))\n",
      "((585, 4096, 9), (585, 4096))\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ALL_FILES)):\n",
    "    f = h5py.File(ALL_FILES[i])\n",
    "    data = f['data'][:]\n",
    "    label = f['label'][:]\n",
    "    print(data.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ALL data\n",
    "data_batch_list = []\n",
    "label_batch_list = []\n",
    "# Add all the h5 files into data_batch_list, and label_batch_list\n",
    "for h5_filename in ALL_FILES:\n",
    "    data_batch, label_batch = provider.loadDataFile(h5_filename)\n",
    "    data_batch_list.append(data_batch)\n",
    "    label_batch_list.append(label_batch) \n",
    "data_batches = np.concatenate(data_batch_list, 0)\n",
    "label_batches = np.concatenate(label_batch_list, 0)\n",
    "print(data_batches.shape)\n",
    "print(label_batches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23585, 4096, 9), (23585, 4096))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batches.shape, label_batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_area = 'Area_'+str(TEST_AREA)\n",
    "train_idxs = []\n",
    "test_idxs = []\n",
    "for i,room_name in enumerate(room_filelist): # 23,585 files\n",
    "    # use one area for testing, and the other five areas for training.\n",
    "    if test_area in room_name:\n",
    "        test_idxs.append(i)\n",
    "    else:\n",
    "        train_idxs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20291, 3294)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_idxs), len(test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((20291, 4096, 9), (20291, 4096))\n",
      "((3294, 4096, 9), (3294, 4096))\n"
     ]
    }
   ],
   "source": [
    "train_data = data_batches[train_idxs,...]\n",
    "train_label = label_batches[train_idxs]\n",
    "test_data = data_batches[test_idxs,...]\n",
    "test_label = label_batches[test_idxs]\n",
    "print(train_data.shape, train_label.shape)\n",
    "print(test_data.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct placeholder\n",
    "def placeholder_inputs(batch_size, num_point):\n",
    "    pointclouds_pl = tf.placeholder(tf.float32,\n",
    "                                     shape=(batch_size, num_point, 9))\n",
    "    labels_pl = tf.placeholder(tf.int32,\n",
    "                                shape=(batch_size, num_point))\n",
    "    return pointclouds_pl, labels_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointclouds_pl, labels_pl = placeholder_inputs(BATCH_SIZE, NUM_POINT) # 24, 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training_pl = tf.placeholder(tf.bool, shape=()) # to determine which stage is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bn_decay(batch):\n",
    "    bn_momentum = tf.train.exponential_decay(\n",
    "                      BN_INIT_DECAY,\n",
    "                      batch*BATCH_SIZE,\n",
    "                      BN_DECAY_DECAY_STEP,\n",
    "                      BN_DECAY_DECAY_RATE,\n",
    "                      staircase=True)\n",
    "    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n",
    "    return bn_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'bn_decay:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the global_step=batch parameter to minimize. \n",
    "# That tells the optimizer to helpfully increment the 'batch' parameter for you every time it trains.\n",
    "batch = tf.Variable(0)\n",
    "bn_decay = get_bn_decay(batch)\n",
    "tf.summary.scalar('bn_decay', bn_decay) # let tensorboard to visualize bn_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 4096)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE, NUM_POINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(24), Dimension(4096), Dimension(9)])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointclouds_pl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 4096, 9)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointclouds_pl.get_shape()[0].value, pointclouds_pl.get_shape()[1].value, pointclouds_pl.get_shape()[2].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = tf.expand_dims(pointclouds_pl, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(24), Dimension(4096), Dimension(9), Dimension(1)])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "# input: pointclouds_pl: 24, 4096, 9, : 24 box, sample 4096 points in each box, \n",
    "#        use 9-dim feature for each point in segmentation task.\n",
    "batch_size = pointclouds_pl.get_shape()[0].value # 24\n",
    "num_point = pointclouds_pl.get_shape()[1].value # 4096\n",
    "\n",
    "input_image = tf.expand_dims(point_cloud, -1) # expand as (24, 4096, 9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONV\n",
    "# kernel_size=[1,9], \"9\" to convolve all 9-dim feature\n",
    "net = tf_util.conv2d(inputs=input_image, num_output_channels=64, kernel_size=[1,9], \n",
    "                     padding='VALID', stride=[1,1], bn=True, is_training=is_training, \n",
    "                     scope='conv1', bn_decay=bn_decay)\n",
    "net = tf_util.conv2d(inputs=net, num_output_channels=64, kernel_size=[1,1], padding='VALID', \n",
    "                     stride=[1,1], bn=True, is_training=is_training, scope='conv2', \n",
    "                     bn_decay=bn_decay)\n",
    "net = tf_util.conv2d(inputs=net, num_output_channels=64, kernel_size=[1,1], padding='VALID', \n",
    "                     stride=[1,1], bn=True, is_training=is_training, scope='conv3', \n",
    "                     bn_decay=bn_decay)\n",
    "net = tf_util.conv2d(inputs=net, num_output_channels=128, kernel_size=[1,1], padding='VALID', \n",
    "                     stride=[1,1], bn=True, is_training=is_training, scope='conv4', \n",
    "                     bn_decay=bn_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-865eeee44838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "type(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 4096)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, num_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model and loss \n",
    "pred = get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay)\n",
    "loss = get_loss(pred, labels_pl)\n",
    "tf.summary.scalar('loss', loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
